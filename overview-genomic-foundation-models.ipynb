{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shivanisharma1297/overview-genomic-foundation-models?scriptVersionId=296707165\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## üß¨ RNA Foundation Models ‚Äî A New Era in Sequence Intelligence\n\nRNA foundation models are large transformer-based architectures trained on massive corpora of nucleotide sequences.\nRather than relying on small labeled datasets, they learn rich biological patterns from billions of RNA and DNA bases, allowing them to internalize:\n\n1. Regulatory signals (promoters, enhancers, splice motifs)\n\n2. Structural motifs (stems, loops, pairing patterns)\n\n3. Evolutionary signatures across species\n\n4. Biochemical properties encoded within sequence context\n\nThese deep, multi-scale representations make them powerful universal feature extractors, enabling high performance on downstream tasks even with limited labeled data.\n\n## üåü Popular RNA/DNA Foundation Models\n\n1. Nucleotide Transformer (InstaDeep) ‚Äî large-scale multi-species training\n\n2. HyenaDNA / Caduceus ‚Äî long-range modeling with linear attention\n\n3. EvoRNA / Evo2-style models ‚Äî structure-aware, evolutionary embeddings\n\n4. DNA-BERT / DNABERT-2 ‚Äî lightweight k-mer transformers for short sequences\n\nIn many ways, these models function like ‚ÄúBERT for Biology‚Äù ‚Äî capturing the grammar, syntax, and semantics of genomic code.","metadata":{}},{"cell_type":"markdown","source":"## üß™ What We Can Do With RNA Foundation Models\n\nThese models can be fine-tuned for almost any sequence-level prediction task:\n\n1. Splice Site Detection: Predict whether a genomic position is a donor/acceptor site.Useful for variant interpretation, transcript annotation, and identifying splicing defects.\n\n2. Promoter & Enhancer Prediction:Detect regulatory regions driving transcription.\n\n3. RNA Secondary Structure Prediction: Predict base-pairing and structural states of RNA molecules.\n\n4. Gene Expression / Regulatory Strength Estimation: Infer promoter strength or translation efficiency from raw sequence.\n\n5. Variant Effect Prediction:Determine whether mutations disrupt regulatory signals or structural motifs.\n\n6.  mRNA Stability / Degradation Signal Prediction:Useful for synthetic biology, mRNA vaccine design.\n\n7.  Long Non-coding RNA (lncRNA) Classification Separate coding from non-coding transcripts.\n\n8.   RNA-binding Protein Site Prediction: Find motifs bound by proteins like AGO, TDP-43, or RBFOX.\n\nüîß Why Fine-Tune Instead of Train From Scratch?\n\nFine-tuning is powerful because:\n\nYou only need a small labeled dataset.\n\nModels already understand biological syntax (motifs, splice codes, kmers).\n\nTraining is faster and cheaper.\n\nPerformance is usually much higher than models trained from scratch.\n\nThis is especially important for tasks like splice-site detection, where classic models need heavy feature engineering. Foundation models skip that ‚Äî they learn the signal automatically","metadata":{}},{"cell_type":"markdown","source":"## üß¨ Understanding RNA Foundation Model Files & How They Work\n\nModern RNA foundation models (like Nucleotide Transformer, HyenaDNA, DNABERT-2, etc.) are stored and distributed through Hugging Face.\nEach model comes with a few important files:\n\n‚úî config.json\n\n1. -Defines the model architecture:\n\n2. -number of layers\n\n3. -hidden size\n\n4. -attention type\n\n5. -tokenizer settings\n\n6. -model type (decoder, encoder, etc.)\n\n‚úî pytorch_model.bin / model.safetensors\n\nThese contain the pretrained weights ‚Äî the actual learned parameters from massive DNA/RNA corpora.\nThis is the core of the foundation model.\n\n‚úî tokenizer.json or tokenizer files\n\nSpecifies:\n\nhow sequences are split (character-level or k-mer like 3-mer, 6-mer)\n\nvocabulary size\n\nspecial tokens\n\n‚úî modeling_*.py (if using trust_remote_code=True)\n\nCustom model architectures (e.g., Hyena, Performer, S4, NT Transformer variants).\n\nWhen you load the model, Hugging Face automatically pulls these files and reconstructs the full architecture.","metadata":{}},{"cell_type":"markdown","source":"üß¨ How We Load an RNA Foundation Model\n\nExample using Nucleotide Transformer 500M:","metadata":{}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    trust_remote_code=True\n)\n","metadata":{}},{"cell_type":"markdown","source":"##  What happens internally?\n\n1. Hugging Face reads the config.json --->Loads the tokenizer (k-mer or raw-token based)---> Reconstructs the model architecture-->Loads pretrained weights into the model--> Adds a classification head if specified (num_labels=2).\n\nThis is identical to loading a pretrained BERT model, just specialized for biology.","metadata":{}},{"cell_type":"markdown","source":"## üß™ Full Fine-Tuning Pipeline (Step-by-Step)\n\nBelow is the full structure of a clean RNA fine-tuning pipeline.\nYou can paste this directly into Markdown in your Kaggle notebook.","metadata":{}},{"cell_type":"markdown","source":"1. Install Dependencies","metadata":{}},{"cell_type":"markdown","source":"!pip install transformers datasets accelerate einops","metadata":{}},{"cell_type":"markdown","source":"2. Load a Dataset (Example: Human Splice Sites)","metadata":{}},{"cell_type":"markdown","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"kentnf/splice_sites_human\")","metadata":{}},{"cell_type":"markdown","source":"3. Load Tokenizer + Model","metadata":{}},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    trust_remote_code=True\n)\n","metadata":{}},{"cell_type":"markdown","source":"4. Preprocessing\n   \ndef preprocess(batch):\n    return tokenizer(\n        batch[\"sequence\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )\n\ntokenized = dataset.map(preprocess, batched=True)\ntokenized = tokenized.rename_column(\"label\", \"labels\")\ntokenized.set_format(\"torch\", [\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{}},{"cell_type":"markdown","source":"5. Training Arguments\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"rna_splicesite\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    load_best_model_at_end=True,\n)","metadata":{}},{"cell_type":"markdown","source":"6. Trainer Setup\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n)\n","metadata":{}},{"cell_type":"markdown","source":"7. Train\n\ntrainer.train()\n","metadata":{}},{"cell_type":"markdown","source":"8. Evaluate\n\nmetrics = trainer.evaluate()\nprint(metrics)","metadata":{}},{"cell_type":"markdown","source":"9. Inference Example\n\n import torch\n\nexample = \"AUGGCUACCUAGGUGAUGGUUUCAUUGGAUGC\"\ninputs = tokenizer(example, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n    pred = torch.argmax(logits, dim=-1).item()\n\nprint(\"Prediction:\", pred)","metadata":{}},{"cell_type":"markdown","source":"## üß¨ Why This Pipeline Works Well\n\nFoundation models already ‚Äúunderstand‚Äù:\n\n- Splice donor/acceptor motifs\n\n- K-mer statistics\n\n- Evolutionary patterns\n\n- RNA structure hints\n\n- Regulatory sequences\n\nFine-tuning only adjusts the top layers and classification head, so:\n\n- Training is fast\n\n- You need fewer labeled data\n\n- Accuracy is high\n\n- Overfitting is reduced","metadata":{}},{"cell_type":"markdown","source":"## üìò Summary of Popular DNA/RNA Foundation Models","metadata":{}},{"cell_type":"markdown","source":"| Model Name                                  | Architecture                  | Max Seq Length   | Tokenization      | Parameters | Strengths                                                                | Good For                                                               |\n| ------------------------------------------- | ----------------------------- | ---------------- | ----------------- | ---------- | ------------------------------------------------------------------------ | ---------------------------------------------------------------------- |\n| **Nucleotide Transformer v2 (500M / 2.5B)** | Transformer Encoder           | 1,000‚Äì2,000      | k-mer (6-mer)     | 500M‚Äì2.5B  | Strong biological representations, trained on huge multi-species dataset | Splice site detection, promoters, enhancers, variant effect prediction |\n| **DNABERT / DNABERT-2**                     | BERT Encoder                  | 512              | k-mer (3‚Äì6-mer)   | 110M‚Äì400M  | Lightweight, easy to fine-tune, excellent for short sequences            | Binary sequence classification, motif detection                        |\n| **Caduceus / HyenaDNA**                     | Hyena + Linear Attention      | 10,000‚Äì1,000,000 | character-level   | 180M‚Äì1B    | Handles extremely long sequences efficiently                             | Long-range regulatory prediction, gene body tasks                      |\n| **Evo2 / EvoRNA / ESM3-like BioLMs**        | Transformer/Hybrid            | 2,048‚Äì32,000     | BPE/AA/RNA tokens | 700M‚Äì7B    | Learns structural + evolutionary signals                                 | RNA structure prediction, RBP binding, folding tasks                   |\n| **Enformer**                                | Transformer + Conv (DeepMind) | 196,608          | character-level   | ~500M      | State-of-the-art on gene regulation prediction                           | Expression prediction, promoter-enhancer interactions                  |\n| **HyenaDNA (Stanford)**                     | Hyena Operator                | 128k‚Äì1M          | character-level   | ~400M      | Scalable to extremely long genomics                                      | Long-range dependency tasks                                            |\n| **GenSLM**                                  | Transformer                   | 2k‚Äì4k            | nucleotide-level  | 500M‚Äì2.5B  | Built for SARS-CoV-2 / viral genomics                                    | Viral lineage classification, mutation impact                          |\n| **GenomeGPT (6B)**                          | Decoder-only LLM              | 2,048            | character-level   | 6B         | Generative and predictive modeling                                       | Synthetic DNA/RNA generation, motif discovery                          |\n","metadata":{}},{"cell_type":"markdown","source":"## üìù How to Use This Table\n\nIf your sequences are short (<512 bp) ‚Üí DNABERT or NT-500M works great.\n\nIf you need long-range biological context (introns, regulatory windows) ‚Üí HyenaDNA or Enformer.\n\nFor RNA structure or interaction prediction ‚Üí EvoRNA / ESM-like models.\n\nFor general-purpose high accuracy ‚Üí NT-2.5B or HyenaDNA (long) + linear attention models.","metadata":{}}]}